{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62a1a26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files found: ['gtp_encapsulation_labeled.csv', 'gtp_malformed_labeled.csv', 'brute_force_attack_labeled.csv', 'intra_upf_ddos_attack_labeled.csv', 'ddos_attack_labeled.csv', 'benign_labeled.csv']\n",
      "Saved training file: training_dataset/training_gtp_encapsulation_labeled.csv\n",
      "Saved training file: training_dataset/training_gtp_malformed_labeled.csv\n",
      "Saved training file: training_dataset/training_brute_force_attack_labeled.csv\n",
      "Saved training file: training_dataset/training_intra_upf_ddos_attack_labeled.csv\n",
      "Saved training file: training_dataset/training_ddos_attack_labeled.csv\n",
      "Saved training file: training_dataset/training_benign_labeled.csv\n",
      "Global Train shape: (53506, 84)\n",
      "Removing 17 highly correlated features: ['Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Fwd Header Length', 'Fwd Packets/s', 'Packet Length Variance', 'PSH Flag Count', 'Average Packet Size', 'Fwd Segment Size Avg', 'Bwd Segment Size Avg', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Max', 'Idle Min']\n",
      "Epoch 1, Loss: 1.8018\n",
      "Epoch 2, Loss: 1.6786\n",
      "Epoch 3, Loss: 1.5583\n",
      "Epoch 4, Loss: 1.4357\n",
      "Epoch 5, Loss: 1.3077\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_0_inference_gtp_encapsulation_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_5e-05_inference_gtp_encapsulation_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_0.05_inference_gtp_encapsulation_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_0.1_inference_gtp_encapsulation_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_0.15_inference_gtp_encapsulation_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_0_inference_gtp_malformed_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_5e-05_inference_gtp_malformed_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_0.05_inference_gtp_malformed_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_0.1_inference_gtp_malformed_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_0.15_inference_gtp_malformed_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_0_inference_brute_force_attack_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_5e-05_inference_brute_force_attack_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_0.05_inference_brute_force_attack_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_0.1_inference_brute_force_attack_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_0.15_inference_brute_force_attack_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_0_inference_intra_upf_ddos_attack_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_5e-05_inference_intra_upf_ddos_attack_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_0.05_inference_intra_upf_ddos_attack_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_0.1_inference_intra_upf_ddos_attack_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_0.15_inference_intra_upf_ddos_attack_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_0_inference_ddos_attack_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_5e-05_inference_ddos_attack_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_0.05_inference_ddos_attack_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_0.1_inference_ddos_attack_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_0.15_inference_ddos_attack_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_0_inference_benign_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_5e-05_inference_benign_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_0.05_inference_benign_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_0.1_inference_benign_labeled.csv\n",
      "Saved adversarial file: adversarial_dataset/adv_eps_0.15_inference_benign_labeled.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --------------------\n",
    "# Config\n",
    "# --------------------\n",
    "original_dataset_folder = \"original_dataset\"\n",
    "training_dataset_folder = \"training_dataset\"\n",
    "adversarial_dataset_folder = \"adversarial_dataset\"\n",
    "\n",
    "os.makedirs(training_dataset_folder, exist_ok=True)\n",
    "os.makedirs(adversarial_dataset_folder, exist_ok=True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "feature_names = None\n",
    "epsilons = [0, 0.00005, 0.05, 0.1, 0.15]\n",
    "\n",
    "# --------------------\n",
    "# Simple NN for FGSM\n",
    "# --------------------\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "def fgsm_attack(data: torch.Tensor, epsilon: float, data_grad: torch.Tensor) -> torch.Tensor:\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    perturbed_data = data + epsilon * sign_data_grad\n",
    "    perturbed_data = torch.clamp(perturbed_data, 0.0, 1.0)\n",
    "    return perturbed_data\n",
    "\n",
    "# --------------------\n",
    "# Utilities\n",
    "# --------------------\n",
    "def remove_highly_correlated_features(df: pd.DataFrame, threshold: float = 0.95) -> pd.DataFrame:\n",
    "    corr_matrix = df.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    if to_drop:\n",
    "        print(f\"Removing {len(to_drop)} highly correlated features: {to_drop}\")\n",
    "    return df.drop(columns=to_drop)\n",
    "\n",
    "def preprocess_data(df: pd.DataFrame, is_training: bool = True):\n",
    "    global feature_names, label_encoder, minmax_scaler\n",
    "\n",
    "    non_feature_cols = ['Flow ID', 'Src IP', 'Dst IP', 'Timestamp', 'source_file']\n",
    "    feature_cols = [col for col in df.columns if col not in non_feature_cols + ['Label']]\n",
    "\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df['Label'].copy()\n",
    "\n",
    "    # Clean labels\n",
    "    y = y.astype(str).str.strip().str.lower().replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "    # Handle missing values\n",
    "    X = X.fillna(X.median(numeric_only=True))\n",
    "\n",
    "    # Convert objects to numeric\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object':\n",
    "            X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "            X[col] = X[col].fillna(X[col].median(numeric_only=True))\n",
    "\n",
    "    # Replace inf\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    X = X.fillna(X.median(numeric_only=True))\n",
    "\n",
    "    if is_training:\n",
    "        X = remove_highly_correlated_features(X, threshold=0.95)\n",
    "        feature_names = list(X.columns)\n",
    "        y_encoded = label_encoder.fit_transform(y)\n",
    "        X_minmax = minmax_scaler.fit_transform(X)\n",
    "    else:\n",
    "        missing_features = set(feature_names) - set(X.columns)\n",
    "        extra_features = set(X.columns) - set(feature_names)\n",
    "\n",
    "        for feat in missing_features:\n",
    "            X[feat] = 0\n",
    "        if extra_features:\n",
    "            X = X.drop(columns=list(extra_features))\n",
    "\n",
    "        X = X[feature_names]\n",
    "\n",
    "        mask = y.isin(label_encoder.classes_)\n",
    "        X = X.loc[mask]\n",
    "        y = y.loc[mask]\n",
    "\n",
    "        y_encoded = label_encoder.transform(y)\n",
    "        X_minmax = minmax_scaler.transform(X)\n",
    "\n",
    "    return X, y, X_minmax, y_encoded\n",
    "\n",
    "def generate_adversarial_data(df: pd.DataFrame, epsilon: float, model: nn.Module, device: str) -> pd.DataFrame:\n",
    "    X_df, y_series, X_minmax, y_encoded = preprocess_data(df, is_training=False)\n",
    "\n",
    "    # Torch tensors\n",
    "    X_tensor = torch.tensor(X_minmax, dtype=torch.float32, device=device, requires_grad=True)\n",
    "    y_tensor = torch.tensor(y_encoded, dtype=torch.long, device=device)\n",
    "\n",
    "    model.zero_grad()\n",
    "    output = model(X_tensor)\n",
    "    loss = F.nll_loss(output, y_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    data_grad = X_tensor.grad.data\n",
    "    perturbed = fgsm_attack(X_tensor, epsilon, data_grad)\n",
    "\n",
    "    perturbed_np = perturbed.detach().cpu().numpy()\n",
    "    X_perturbed_orig = minmax_scaler.inverse_transform(perturbed_np)\n",
    "\n",
    "    perturbed_df = df.loc[X_df.index].copy()\n",
    "    perturbed_df[feature_names] = X_perturbed_orig\n",
    "    return perturbed_df\n",
    "\n",
    "# --------------------\n",
    "# 1) Load, split, and save per file\n",
    "# --------------------\n",
    "train_parts = []\n",
    "infer_parts = []\n",
    "\n",
    "csv_files = [f for f in os.listdir(original_dataset_folder) if f.endswith('.csv')]\n",
    "print(\"CSV files found:\", csv_files)\n",
    "\n",
    "for f in csv_files:\n",
    "    df = pd.read_csv(os.path.join(original_dataset_folder, f)).drop_duplicates()\n",
    "\n",
    "    train_df, infer_df = train_test_split(\n",
    "        df,\n",
    "        test_size=0.20,\n",
    "        stratify=df[\"Label\"],\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Save training split immediately\n",
    "    train_outfile = os.path.join(training_dataset_folder, f\"training_{f}\")\n",
    "    train_df.to_csv(train_outfile, index=False)\n",
    "    print(f\"Saved training file: {train_outfile}\")\n",
    "\n",
    "    train_parts.append(train_df)\n",
    "    infer_parts.append((f, infer_df))  # keep filename for later adversarial gen\n",
    "\n",
    "# Merge all train parts for preprocessing & NN training\n",
    "train_df = pd.concat(train_parts, ignore_index=True)\n",
    "print(\"Global Train shape:\", train_df.shape)\n",
    "\n",
    "# --------------------\n",
    "# 2) Preprocess training data\n",
    "# --------------------\n",
    "X_train_df, y_train_series, X_train_minmax, y_train_encoded = preprocess_data(train_df, is_training=True)\n",
    "\n",
    "# --------------------\n",
    "# 3) Train NN\n",
    "# --------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_dim = X_train_minmax.shape[1]\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "nn_model = SimpleNet(input_dim=input_dim, num_classes=num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(nn_model.parameters(), lr=0.01)\n",
    "\n",
    "nn_model.train()\n",
    "X_tensor = torch.tensor(X_train_minmax, dtype=torch.float32, device=device)\n",
    "y_tensor = torch.tensor(y_train_encoded, dtype=torch.long, device=device)\n",
    "for epoch in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    out = nn_model(X_tensor)\n",
    "    loss = F.nll_loss(out, y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "nn_model.eval()\n",
    "\n",
    "# --------------------\n",
    "# 4) Generate adversarial inference data per file\n",
    "# --------------------\n",
    "for f, infer_df in infer_parts:\n",
    "    for eps in epsilons:\n",
    "        adv_df = generate_adversarial_data(infer_df, eps, nn_model, device)\n",
    "        outfile = os.path.join(adversarial_dataset_folder, f\"adv_eps_{eps}_inference_{f}\")\n",
    "        adv_df.to_csv(outfile, index=False)\n",
    "        print(f\"Saved adversarial file: {outfile}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
